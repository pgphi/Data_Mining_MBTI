{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e60bac",
   "metadata": {},
   "source": [
    "# Linear Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc714f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn import feature_extraction, feature_selection, model_selection\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a879a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>encodedType</th>\n",
       "      <th>preprocessed_posts</th>\n",
       "      <th>extro_intro</th>\n",
       "      <th>intu_obs</th>\n",
       "      <th>feel_think</th>\n",
       "      <th>prosp_judg</th>\n",
       "      <th>avg_word_count</th>\n",
       "      <th>avg_exclam_count</th>\n",
       "      <th>avg_fullstop_count</th>\n",
       "      <th>avg_emoji_count</th>\n",
       "      <th>avg_count_of_hello</th>\n",
       "      <th>avg_count_of_hi</th>\n",
       "      <th>avg_count_of_extroverted_bigrams</th>\n",
       "      <th>avg_count_of_extroverted_stylistic_impressions</th>\n",
       "      <th>avg_count_of_interoverted_quantifiers</th>\n",
       "      <th>avg_count_of_introverted_first_person_singular_pronoun</th>\n",
       "      <th>avg_count_of_introverted_negations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>8</td>\n",
       "      <td>['youtube', 'tumblr', 'enfp', 'intj', 'moment'...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>3</td>\n",
       "      <td>['im', 'finding', 'the', 'lack', 'of', 'post',...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.58</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>11</td>\n",
       "      <td>['good', 'one', 'youtube', 'of', 'course', 'i'...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.18</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>10</td>\n",
       "      <td>['dear', 'intp', 'i', 'enjoyed', 'conversation...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>2</td>\n",
       "      <td>['youre', 'fired', 'thats', 'another', 'silly'...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  encodedType  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...            8   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...            3   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...           11   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...           10   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...            2   \n",
       "\n",
       "                                  preprocessed_posts  extro_intro  intu_obs  \\\n",
       "0  ['youtube', 'tumblr', 'enfp', 'intj', 'moment'...            0         1   \n",
       "1  ['im', 'finding', 'the', 'lack', 'of', 'post',...            1         1   \n",
       "2  ['good', 'one', 'youtube', 'of', 'course', 'i'...            0         1   \n",
       "3  ['dear', 'intp', 'i', 'enjoyed', 'conversation...            0         1   \n",
       "4  ['youre', 'fired', 'thats', 'another', 'silly'...            1         1   \n",
       "\n",
       "   feel_think  prosp_judg  avg_word_count  avg_exclam_count  \\\n",
       "0           1           0           12.10               0.0   \n",
       "1           0           1           24.38               0.0   \n",
       "2           0           1           17.70               0.0   \n",
       "3           0           0           22.26               0.0   \n",
       "4           0           0           20.32               0.0   \n",
       "\n",
       "   avg_fullstop_count  avg_emoji_count  avg_count_of_hello  avg_count_of_hi  \\\n",
       "0                0.16             0.08                 0.0              0.0   \n",
       "1                0.04             0.08                 0.0              0.0   \n",
       "2                0.08             0.00                 0.0              0.0   \n",
       "3                0.14             0.02                 0.0              0.0   \n",
       "4                0.08             0.06                 0.0              0.0   \n",
       "\n",
       "   avg_count_of_extroverted_bigrams  \\\n",
       "0                              0.14   \n",
       "1                              0.28   \n",
       "2                              0.26   \n",
       "3                              0.20   \n",
       "4                              0.14   \n",
       "\n",
       "   avg_count_of_extroverted_stylistic_impressions  \\\n",
       "0                                             0.0   \n",
       "1                                             0.0   \n",
       "2                                             0.0   \n",
       "3                                             0.0   \n",
       "4                                             0.0   \n",
       "\n",
       "   avg_count_of_interoverted_quantifiers  \\\n",
       "0                                   0.06   \n",
       "1                                   0.14   \n",
       "2                                   0.00   \n",
       "3                                   0.10   \n",
       "4                                   0.00   \n",
       "\n",
       "   avg_count_of_introverted_first_person_singular_pronoun  \\\n",
       "0                                               3.54        \n",
       "1                                               5.58        \n",
       "2                                               5.18        \n",
       "3                                               6.12        \n",
       "4                                               6.10        \n",
       "\n",
       "   avg_count_of_introverted_negations  \n",
       "0                                0.02  \n",
       "1                                0.10  \n",
       "2                                0.04  \n",
       "3                                0.10  \n",
       "4                                0.10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data into a dataset\n",
    "df = pd.read_csv(\"mbit_preprocessed_features.csv\", index_col=0)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc1522ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train/test split\n",
    "def train_test_split(df, test_size, rs, balancing, binary):\n",
    "    \"\"\"\n",
    "    :param balancing: if true return balanced train set\n",
    "    :param binary: if true return 0 (extroverted) and 1 (introverted) classes\n",
    "    :param df: (preprocessed) dataset\n",
    "    :param test_size: choose train/test split ratio <float>\n",
    "    :param rs: choose random state <float>\n",
    "    :return: Train and Test Split with and without encoding of <string> labels\n",
    "    \"\"\"\n",
    "\n",
    "    enc = OrdinalEncoder()\n",
    "    df[\"encoded_types\"] = enc.fit_transform(df[[\"type\"]])\n",
    "    # print(\"Raw Dataset:\")\n",
    "    # print(df)\n",
    "\n",
    "    if binary:\n",
    "\n",
    "        df = df.replace({\"type\": {\"INTJ\": \"Introverted\",\n",
    "                                \"INTP\": \"Introverted\",\n",
    "                                \"ENTJ\": \"Extroverted\",\n",
    "                                \"ENTP\": \"Extroverted\",\n",
    "                                \"INFJ\": \"Introverted\",\n",
    "                                \"INFP\": \"Introverted\",\n",
    "                                \"ENFJ\": \"Extroverted\",\n",
    "                                \"ENFP\": \"Extroverted\",\n",
    "                                \"ISTJ\": \"Introverted\",\n",
    "                                \"ISFJ\": \"Introverted\",\n",
    "                                \"ESTJ\": \"Extroverted\",\n",
    "                                \"ESFJ\": \"Extroverted\",\n",
    "                                \"ISTP\": \"Introverted\",\n",
    "                                \"ISFP\": \"Introverted\",\n",
    "                                \"ESTP\": \"Extroverted\",\n",
    "                                \"ESFP\": \"Extroverted\"}})\n",
    "\n",
    "\n",
    "\n",
    "    # Create training and test split\n",
    "    ## get X\n",
    "    X_train, X_test = model_selection.train_test_split(df, test_size=test_size, random_state=rs)\n",
    "    # print(\"X Training Set:\" + \"länge=\" + str(len(X_train)))\n",
    "    # print(X_train)\n",
    "    # print(\"X Test Set:\" + \"länge=\" + str(len(X_test)))\n",
    "    # print(X_test)\n",
    "\n",
    "    ## get target\n",
    "    y_train = X_train[\"encodedType\"].values\n",
    "    print(\"Y Train Set:\" + \"länge=\" + str(len(y_train)))\n",
    "    # print(y_train)\n",
    "    y_test = X_test[\"encodedType\"].values\n",
    "    print(\"Y Test Set:\" + \"länge=\" + str(len(y_test)))\n",
    "    # print(y_test)\n",
    "\n",
    "    # Balancing\n",
    "    if balancing:\n",
    "        oversample = RandomOverSampler(sampling_strategy=\"minority\")\n",
    "        X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "        X_train = X_over\n",
    "        y_train = y_over\n",
    "\n",
    "    else:\n",
    "        None\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def feature_generator(X_train, X_test, y_train, n_gram, p_value):\n",
    "    ## TF-IDF (advanced variant of BoW)\n",
    "    TFIDF = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1, n_gram))\n",
    "\n",
    "    ##Extract Vocabulary\n",
    "    corpus = X_train[\"preprocessed_posts\"].values.astype(str)\n",
    "    TFIDF.fit(corpus)\n",
    "    X_training = TFIDF.transform(corpus)\n",
    "    dic_vocabulary = TFIDF.vocabulary_\n",
    "    # print(\"Training vocabulary size before dimension reduction: \" + str(len(dic_vocabulary)))\n",
    "\n",
    "    ##Look up Position of a certain word in the Sparse Matrix\n",
    "    word = \"think\"\n",
    "    # print(\"Position of the word \" + word + \" in matrix: \" + str(dic_vocabulary[word]))\n",
    "\n",
    "    # Feature Selection\n",
    "\n",
    "    ##Reduce Dimensionality for sparse data with Chi-Quadrat\n",
    "    X_names = TFIDF.get_feature_names_out()\n",
    "    p_value_limit = p_value\n",
    "    features = pd.DataFrame()\n",
    "    # print(\"Top Features for Each Class:\")\n",
    "    for cat in np.unique(y_train):\n",
    "        chi2, p = feature_selection.chi2(X_training, y_train == cat)\n",
    "        features = features.append(pd.DataFrame(\n",
    "            {\"feature\": X_names, \"score\": 1 - p, \"y\": cat}))\n",
    "        features = features.sort_values([\"y\", \"score\"], ascending=[True, False])\n",
    "        features = features[features[\"score\"] > p_value_limit]\n",
    "    X_names = features[\"feature\"].unique().tolist()\n",
    "    print(X_names)\n",
    "    print(len(X_names))\n",
    "\n",
    "    for cat in np.unique(y_train):\n",
    "        print(\"# {}:\".format(cat))\n",
    "        print(\"  . selected features:\",\n",
    "              len(features[features[\"y\"] == cat]))\n",
    "        print(\"  . top features:\", \",\".join(\n",
    "            features[features[\"y\"] == cat][\"feature\"].values[:10]))\n",
    "        print(\" \")\n",
    "\n",
    "    ##Re-Fit vectorizer on corpus with new set of words and create new sparse matrix\n",
    "    TFIDF = feature_extraction.text.TfidfVectorizer(vocabulary=X_names)\n",
    "    TFIDF.fit(corpus)\n",
    "    X_train_vec = TFIDF.transform(corpus)\n",
    "    dic_vocabulary = TFIDF.vocabulary_\n",
    "    print(\"Training vocabulary size after dimension reduction: \" + str(len(dic_vocabulary)))\n",
    "\n",
    "    return X_train_vec, None, TFIDF\n",
    "\n",
    "# Train the model and print the classification report\n",
    "def train_predict_model(X, Y, types, show_matrix):\n",
    "    X_train, X_test, Y_train, Y_test = create_train_test_split(X, Y)\n",
    "    \n",
    "    # Train the model\n",
    "    estimator = LinearSVC()\n",
    "\n",
    "    estimator.fit(X_train, Y_train)\n",
    "    \n",
    "    # Prediction\n",
    "    predicted = estimator.predict(X_test)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(Y_test, predicted, target_names=types, zero_division=0))\n",
    "    \n",
    "    if show_matrix:\n",
    "        cnf_matrix = confusion_matrix(Y_test, predicted)\n",
    "        np.set_printoptions(precision=2)\n",
    "        plot_confusion_matrix(cnf_matrix, classes=types)\n",
    "        \n",
    "    return predicted\n",
    "\n",
    "# Plot the confusion matrix (copied from Exercise 3)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52ba580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Train Set:länge=6072\n",
      "Y Test Set:länge=2603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n",
      "C:\\Users\\AGANDO\\AppData\\Local\\Temp\\ipykernel_7108\\3076623148.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(pd.DataFrame(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enfj', 'enfjs', 'the enfj', 'enfj the', 'seeker', 'welcome welcome', 'thanks input', 'giggle', 'enfp', 'enfps', 'im enfp', 'am enfp', 'the enfp', 'emojid', 'of enfps', 'xd', 'enfps the', 'enfp im', 'of enfp', 'the enfps', 'sosx', 'be enfp', 'enfp the', 'entj', 'entjs', 'the entj', 'entj the', 'entp', 'entps', 'im entp', 'of entps', 'the entp', 'entp im', 'entp the', 'of entp', 'entps the', 'ne', 'be entp', 'parrot', 'esfjs', 'esfj', 'the esfj', 'esfp', 'sims', 'esfps', 'the esfp', 'stark', 'tmlt', 'estjs', 'estj', 'eagle', 'wat', 'ap', 'pony', 'estp', 'estps', 'the estp', 'cough', 'estp the', 'infj', 'infjs', 'im infj', 'am infj', 'the infj', 'of infj', 'youre infj', 'another infj', 'of infjs', 'think infjs', 'know infj', 'type infj', 'infj can', 'infj the', 'ni', 'be infj', 'infjs the', 'infj forum', 'infj type', 'infj would', 'infj male', 'infj not', 'infj dont', 'dear', 'think infj', 'infp', 'infps', 'im infp', 'am infp', 'the infp', 'of infps', 'youre infp', 'of infp', 'think infps', 'infps can', 'infp male', 'infp thing', 'intj', 'intp', 'many infps', 'infp can', 'infp forum', 'another infp', 'infp im', 'be infp', 'feel', 'infp the', 'think infp', 'infp not', 'infp am', 'infp like', 'know infp', 'istp', 'intjs', 'intps', 'im intj', 'the intj', 'am intj', 'intj woman', 'type intj', 'intjs do', 'of intjs', 'intj female', 'of intj', 'intj the', 'think intjs', 'intj forum', 'intjs dont', 'intj like', 'intj not', 'intjs can', 'intj would', 'intjs the', 'intj im', 'im intp', 'the intp', 'am intp', 'youre intp', 'intp female', 'of intps', 'of intp', 'intp forum', 'think intps', 'be intp', 'intp the', 'type intp', 'intp dont', 'intp would', 'intp im', 'physic', 'intps the', 'intp can', 'the intps', 'inferior fe', 'universe', 'im isfj', 'isfj', 'isfjs', 'the isfj', 'teddy', 'hello welcome', 'si', 'isfp', 'isfps', 'im isfp', 'the isfp', 'drawing', 'istj', 'istjs', 'the istj', 'rant', 'istj the', 'snape', 'istps', 'im istp', 'the istp', 'istp the', 'mechanic']\n",
      "177\n",
      "# 0:\n",
      "  . selected features: 8\n",
      "  . top features: enfj,enfjs,the enfj,enfj the,seeker,welcome welcome,thanks input,giggle\n",
      " \n",
      "# 1:\n",
      "  . selected features: 15\n",
      "  . top features: enfp,enfps,im enfp,am enfp,the enfp,emojid,of enfps,xd,enfps the,enfp im\n",
      " \n",
      "# 2:\n",
      "  . selected features: 4\n",
      "  . top features: entj,entjs,the entj,entj the\n",
      " \n",
      "# 3:\n",
      "  . selected features: 12\n",
      "  . top features: entp,entps,im entp,of entps,the entp,entp im,entp the,of entp,entps the,ne\n",
      " \n",
      "# 4:\n",
      "  . selected features: 3\n",
      "  . top features: esfjs,esfj,the esfj\n",
      " \n",
      "# 5:\n",
      "  . selected features: 6\n",
      "  . top features: esfp,sims,esfps,the esfp,stark,tmlt\n",
      " \n",
      "# 6:\n",
      "  . selected features: 6\n",
      "  . top features: estjs,estj,eagle,wat,ap,pony\n",
      " \n",
      "# 7:\n",
      "  . selected features: 5\n",
      "  . top features: estp,estps,the estp,cough,estp the\n",
      " \n",
      "# 8:\n",
      "  . selected features: 25\n",
      "  . top features: infj,infjs,im infj,am infj,the infj,of infj,youre infj,another infj,of infjs,think infjs\n",
      " \n",
      "# 9:\n",
      "  . selected features: 35\n",
      "  . top features: infp,infps,im infp,am infp,the infp,of infps,youre infp,of infp,think infps,entp\n",
      " \n",
      "# 10:\n",
      "  . selected features: 22\n",
      "  . top features: intj,intjs,im intj,the intj,am intj,intj woman,type intj,intjs do,of intjs,intj female\n",
      " \n",
      "# 11:\n",
      "  . selected features: 27\n",
      "  . top features: intp,intps,im intp,the intp,am intp,youre intp,intp female,of intps,of intp,infj\n",
      " \n",
      "# 12:\n",
      "  . selected features: 7\n",
      "  . top features: im isfj,isfj,isfjs,the isfj,teddy,hello welcome,si\n",
      " \n",
      "# 13:\n",
      "  . selected features: 5\n",
      "  . top features: isfp,isfps,im isfp,the isfp,drawing\n",
      " \n",
      "# 14:\n",
      "  . selected features: 6\n",
      "  . top features: istj,istjs,the istj,rant,istj the,snape\n",
      " \n",
      "# 15:\n",
      "  . selected features: 6\n",
      "  . top features: istp,istps,im istp,the istp,istp the,mechanic\n",
      " \n",
      "Training vocabulary size after dimension reduction: 177\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test_base, y_train, y_test = train_test_split(df=df, test_size=0.3, rs=42069, \n",
    "                                                                             balancing=False, binary=False)\n",
    "X_train_vec, none, vectorizer = feature_generator(X_train, X_test_base, y_train, 2, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3fe91",
   "metadata": {},
   "source": [
    "### Multi-class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eae9add2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ENFJ       0.49      0.40      0.44        55\n",
      "        ENFP       0.63      0.69      0.66       212\n",
      "        ENTJ       0.67      0.61      0.64        74\n",
      "        ENTP       0.63      0.65      0.64       196\n",
      "        ESFJ       0.50      0.45      0.48        11\n",
      "        ESFP       0.33      0.07      0.12        14\n",
      "        ESTJ       0.86      0.55      0.67        11\n",
      "        ESTP       0.64      0.50      0.56        28\n",
      "        INFJ       0.71      0.71      0.71       436\n",
      "        INFP       0.71      0.77      0.74       545\n",
      "        INTJ       0.71      0.58      0.64       365\n",
      "        INTP       0.69      0.74      0.72       378\n",
      "        ISFJ       0.68      0.71      0.69        58\n",
      "        ISFP       0.57      0.53      0.55        73\n",
      "        ISTJ       0.64      0.67      0.66        61\n",
      "        ISTP       0.64      0.72      0.68        86\n",
      "\n",
      "    accuracy                           0.68      2603\n",
      "   macro avg       0.63      0.58      0.60      2603\n",
      "weighted avg       0.68      0.68      0.68      2603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-class model\n",
    "types = sorted(list(set(df.iloc[:, 0].values)))\n",
    "\n",
    "# Train the model\n",
    "estimator = LinearSVC()\n",
    "estimator.fit(X_train_vec, y_train)\n",
    "    \n",
    "# Prediction\n",
    "predicted = estimator.predict(vectorizer.transform(X_test_base[\"preprocessed_posts\"]))\n",
    "    \n",
    "# Evaluation\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predicted, target_names=types, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e5957",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea4f49a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "40 fits failed out of a total of 80.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1204, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1043, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1204, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1043, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1204, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1043, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1204, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1043, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\AGANDO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.67836616        nan 0.684459          nan        nan\n",
      " 0.6842948  0.68462374]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dual': False, 'loss': 'squared_hinge', 'penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gridsearch\n",
    "\n",
    "parameters = {       \n",
    "'penalty': ['l1', 'l2'],\n",
    "'loss': ['hinge', 'squared_hinge'],\n",
    "'dual': [True, False],\n",
    "}\n",
    "\n",
    "estimator = LinearSVC(max_iter=15000)\n",
    "\n",
    "# Because of reduced data -> 10 folds instead of 3\n",
    "stratified_10_fold_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "grid_search_estimator = GridSearchCV(estimator, parameters, scoring='accuracy', cv=stratified_10_fold_cv)\n",
    "\n",
    "grid_search_estimator.fit(X_train_vec, y_train)\n",
    "display(grid_search_estimator.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ba365",
   "metadata": {},
   "source": [
    "### Multi-class model (with found parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63d2e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ENFJ       0.49      0.40      0.44        55\n",
      "        ENFP       0.63      0.69      0.66       212\n",
      "        ENTJ       0.67      0.61      0.64        74\n",
      "        ENTP       0.63      0.65      0.64       196\n",
      "        ESFJ       0.50      0.45      0.48        11\n",
      "        ESFP       0.33      0.07      0.12        14\n",
      "        ESTJ       0.86      0.55      0.67        11\n",
      "        ESTP       0.64      0.50      0.56        28\n",
      "        INFJ       0.71      0.71      0.71       436\n",
      "        INFP       0.71      0.77      0.74       545\n",
      "        INTJ       0.71      0.58      0.64       365\n",
      "        INTP       0.69      0.74      0.72       378\n",
      "        ISFJ       0.68      0.71      0.69        58\n",
      "        ISFP       0.57      0.53      0.55        73\n",
      "        ISTJ       0.64      0.67      0.66        61\n",
      "        ISTP       0.64      0.72      0.68        86\n",
      "\n",
      "    accuracy                           0.68      2603\n",
      "   macro avg       0.63      0.58      0.60      2603\n",
      "weighted avg       0.68      0.68      0.68      2603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator = LinearSVC(dual=False)\n",
    "estimator.fit(X_train_vec, y_train)\n",
    "    \n",
    "# Prediction\n",
    "predicted = estimator.predict(vectorizer.transform(X_test_base[\"preprocessed_posts\"]))\n",
    "    \n",
    "# Evaluation\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predicted, target_names=types, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb89a280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
